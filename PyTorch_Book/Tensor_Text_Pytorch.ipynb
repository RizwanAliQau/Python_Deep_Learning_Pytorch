{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has taken the field of natural language processing (NLP) by storm, particularly by using models that repeatedly consume a combination of new input and\n",
    " previous model output. These models are called recurrent neural networks, and they’ve\n",
    " been applied with great success to text categorization, text generation, and automated\n",
    " translation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Previous NLP workloads were characterized by sophisticated multistage pipelines that included rules encoding the grammar of a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-of-the-art work trains networks end to end on large corpuses starting from\n",
    " scratch, letting those rules emerge from data. For the past several years, the most-used\n",
    " automated translation systems available as services on the internet have been based on\n",
    " deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Networks operate on text at two levels: at character level, by processing one character at a time, and at word level, in which individual words are the finest-grained entities seen by the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "The technique you use to encode text information into\n",
    " tensor form is the same whether you operate at character level or at word level. It’s one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with a character-level example. First, get some text to process. An amazing resource is Project Gutenberg, a volunteer effort that digitizes and archives cultural work and makes it available for free in open formats, including plain-text files.\n",
    "\n",
    "#### Note:\n",
    "If you’re aiming at larger-scale corpora, the Wikipedia corpus stands out: it’s the\n",
    " complete collection of Wikipedia articles containing 1.9 billion words and more\n",
    " than 4.4 million articles. You can find several other corpora at the English Corpora\n",
    " website.\n",
    " \n",
    " http://www.gutenberg.org \n",
    " https://www.english-corpora.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load Jane Austen’s Pride and Prejudice from the Project Gutenberg website.Save\n",
    " the file and read it in, as shown in the following listing.\n",
    "http://www.gutenberg.org/files/1342/1342-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Haier/Desktop/Sch_Apply.txt', encoding='utf8') as f:   \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding:  ASCII encodes 128 characters\n",
    " using 128 integers. Letter a, for example, corresponds to binary 1100001 or decimal\n",
    " 97; letter b corresponds to binary 1100010 or decimal 98, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE:\n",
    "Clearly, 128 characters aren’t enough to account for all the glyphs,\n",
    " accents, ligatures, and other features that are needed to properly represent\n",
    " written text in languages other than English. To this end, other encodings\n",
    " have been developed, using a larger number of bits as a code for a wider\n",
    " range of characters. That wider range of characters got standardized as Unicode, which maps all known characters to numbers, with the representation in\n",
    " bits of those numbers being provided by a specific encoding. Popular encodings include UTF-8, UTF-16 and UTF-32, in which the numbers are a sequence\n",
    " of 8-, 16-, or 32-bit integers. Strings in Python 3.x are Unicode strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you need to parse the characters in the text and provide a one-hot encoding for each of them. Each character will be represented by a vector of length equal to the\n",
    " number of characters in the encoding. This vector will contain all zeros except for a 1 at\n",
    " the index corresponding to the location of the character in the encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">First, split your text into a list of lines and pick an arbitrary line to focus on:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instead of doing analysis, we make a pre-supposition that the particular method will not work well. We are more inclined to the hype created by NN. '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split('\\n') \n",
    "line = lines[2] \n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a tensor that can hold the total number of one-hot encoded characters for the\n",
    " whole line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([148, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "letter_tensor = torch.zeros(len(line), 128) # 128 hardcoded due to the \n",
    "#limits of ASCII\\\n",
    "letter_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that letter_tensor holds a one-hot encoded character per row. Now set a 1 on\n",
    " each row in the right position so that each row represents the right character. The index\n",
    " where the 1 has to be set corresponds to the index of the character in the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()): \n",
    "    \n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0 \n",
    "    letter_tensor[i][letter_index] = 1\n",
    "\n",
    "# 128 hardcoded due to the limits of ASCII\\ The text uses directional\n",
    "# double quotes , which aren’t valid ASCII, so screen them out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i\n",
      "105\n",
      "1 n\n",
      "110\n",
      "2 s\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "## what the above function is doing \n",
    "for i,l in enumerate(line.lower().strip()):\n",
    "    if i<3:\n",
    "        print(i,l)\n",
    "        print(ord(l)) # ascii in decimal conversion \n",
    "    else:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve one-hot encoded your sentence into a representation that a neural network\n",
    " can digest. You could do word-level encoding the same way by establishing a vocabulary and one-hot encoding sentences, sequences of words, along the rows of your tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "A vocabulary contains many words, this method produces wide encoded\n",
    " vectors that may not be practical. Later in this chapter, you see a more efficient way to\n",
    " represent text at word level by using embeddings. For now, stick with one-hot encodings to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define clean_words, which takes text and returns it lowercase and stripped of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(input_str): \n",
    "    punctuation = '.,;:\"!?”“_-'   \n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list] # list comp\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '\"cde']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_l = 'abc \"cde'.split()\n",
    "word_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('abc cde'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cde'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_l[1].strip('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"cde', 'abc'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(word_l) # convert into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"cde', 'abc']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(word_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Instead of doing analysis, we make a pre-supposition that the particular method will not work well. We are more inclined to the hype created by NN. ',\n",
       " ['instead', 'of', 'doing', 'analysis'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean data \n",
    "words_in_line = clean_words(line)\n",
    "line, words_in_line[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, build a mapping of words to indexes in your encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 224)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
    "len(word2index_dict), word2index_dict['nn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all words is now a dictionary with words as keys and an integer as value.You’ll use this dictionary to efficiently find the index of a word as you one-hot encode it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now focus on your sentence. Break it into words and one-hot encode it—that is,\n",
    " populate a tensor with one one-hot encoded vector per word. Create an empty vector,\n",
    " and assign the one-hot encoded values of the word in the sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  174 instead\n",
      " 1  229 of\n",
      " 2  103 doing\n",
      " 3   25 analysis\n",
      " 4  371 we\n",
      " 5  196 make\n",
      " 6    6 a\n",
      " 7  257 pre-supposition\n",
      " 8  336 that\n",
      " 9  337 the\n",
      "10  245 particular\n",
      "11  206 method\n",
      "12  381 will\n",
      "13  226 not\n",
      "14  384 work\n",
      "15  374 well\n",
      "16  371 we\n",
      "17   38 are\n",
      "18  214 more\n",
      "19  171 inclined\n",
      "20  352 to\n",
      "21  337 the\n",
      "22  162 hype\n",
      "23   88 created\n",
      "24   56 by\n",
      "25  224 nn\n"
     ]
    }
   ],
   "source": [
    "word_tensor = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "for i, word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word] \n",
    "    word_tensor[i][word_index] = 1 \n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 388])\n"
     ]
    }
   ],
   "source": [
    "print(word_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "At this point, tensor represents one sentence of length 26 in an encoding space of size 388—the number of words in your dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
